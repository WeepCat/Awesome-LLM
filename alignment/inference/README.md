# TEST-TIME ALIGNMENT

Traditional training-time methods fine-tune LLMs using human preference datasets but incur significant training costs 
and require repeated training to handle diverse user preferences. Test-time alignment methods address this by using 
reward models (RMs) to guide frozen LLMs without retraining. [--GENARM: REWARD GUIDED GENERATION WITH AUTOREGRESSIVE REWARD MODEL FOR TEST-TIME ALIGNMENT](http://arxiv.org/abs/2410.08193)
